# lightning.pytorch==2.2.0.post0
seed_everything: 0
trainer:
  max_epochs: 4
  sync_batchnorm: false
  val_check_interval: 0.5
  precision: 16-mixed
  gradient_clip_val: 10.0
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        save_top_k: -1
        save_on_train_epoch_end: true
        monitor: val/loss
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
model:
  network: resnet10_relu
  val_lambda: 0.333
  val_lambda_decay_epoch: 3
  use_ema: false
  update_bn: false
  ema_start_epoch: 1
  ema_freq: 250
  ema_decay: 0.9
  model_filename: model-{epoch:03}
  lr_scheduler_interval: step
data:
  train_files:
    - F:\hcpe3\floodgate_2022_0214_r3800_nomate.hcpe3
  val_files:
    - F:\hcpe3\floodgate.hcpe
  batch_size: 1024
  val_batch_size: 1024
  use_average: true
  use_evalfix: true
  temperature: 1.0
  cache: null
optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 1e-4
    betas:
    - 0.9
    - 0.999
    eps: 1e-08
    weight_decay: 1e-2
lr_scheduler:
  class_path: dlshogi.lr_scheduler.CosineLRScheduler
  init_args:
    t_initial: 100
    lr_min: 1e-8
    cycle_mul: 2
    cycle_limit: 8
    cycle_decay: 0.5
    warmup_t: 10
    warmup_lr_init: 1e-7
    warmup_prefix: true
